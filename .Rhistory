shiny::runApp('C:/Users/Nick/Dropbox/Work/Job Search/2016 - JOB SEARCH/Shiny Resume/shiny resume')
install.packages("shinythemes")
install.packages("leaflet")
setwd('C:\\Users\\Nick\\Dropbox\\Work\\Data Science\\13 - Data for Good Datathon\\scripts')
topicmodels_json_ldavis
source('C:/Users/Nick/Dropbox/Work/Data Science/13 - Data for Good Datathon/scripts/topicmodels_json_ldavis.R')
source("LDA.analysis.R")
cleanFun("Hello<>")
cleanFun2("Hello<>")
source("LDA.analysis.R")
source("topicmodels_json_ldavis.R")
library(LDAvis)
install.packages("LDAvis")
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
library(tm)
setwd('C:\\Users\\Nick\\Dropbox\\Work\\Data Science\\13 - Data for Good Datathon\\scripts')
source("LDA.analysis.R")
source("topicmodels_json_ldavis.R")
df=read.csv('ckc_calgary_mega_export.txt', sep = "\t",
stringsAsFactors = FALSE)
#Clean Data
#Pull only columns of interest into a new dataframe
dfn=df['nid']
dfn$Title=df$Title
dfn$WhyWeExist=trimws(cleanFun2(cleanFun(df$Why.We.Exist)), which = "both")
dfn$WhatWeDo=trimws(cleanFun2(cleanFun(df$What.We.Do)), which = "both")
dfn$HowWeDoIT=trimws(cleanFun2(cleanFun(df$How.We.Do.It)), which = "both")
dfn$WhatYouCanDo=trimws(cleanFun2(cleanFun(df$What.You.Can.Do)), which = "both")
dfn$Website=df$website
dfn$Email=df$email
dfn$field_charitable_number_value=df$field_charitable_number_value
dfn$Phone=df$field_org_phone_value
dfn$Representative=df$Representative
dfn$Annual_budget=df$field_org_annual_budget_value
dfnew=paste(dfn$WhyWeExist,
dfn$WhatWeDo,
dfn$HowWeDoIT,
sep=" ")
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
#Create labels for topics  - join 3 most common words
library(tidyr)
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
#system.time(
llis.model <- topicmodels::LDA(llisreduced.dtm,
optimal_topic_number,
method = "Gibbs",
control = list(iter=2000, seed = 0622))
#system.time(
llisreduced.dtm <- dtm[,term_tfidf >= 0.005]
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
#system.time(
term_tfidf <- tapply(dtm$v/slam::row_sums(dtm)[dtm$i], dtm$j, mean) *log2(tm::nDocs(dtm)/slam::col_sums(dtm > 0))
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
docs <- Corpus(VectorSource(text_vector))
#removes &...; string
cleanFun2 <- function(htmlString) {
return(gsub("&.*?;", "", htmlString))
}
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
source("LDA.analysis.R")
source("topicmodels_json_ldavis.R")
debugSource('C:/Users/Nick/Dropbox/Work/Data Science/13 - Data for Good Datathon/scripts/LDA.analysis.R')
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
term_tfidf
debugSource('C:/Users/Nick/Dropbox/Work/Data Science/13 - Data for Good Datathon/scripts/LDA.analysis.R')
llis.model <- topicmodels::LDA(llisreduced.dtm,
num_topics,
method = "Gibbs",
control = list(iter=2000, seed = 0622))
source("LDA.analysis.R")
source("topicmodels_json_ldavis.R")
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
debugSource('C:/Users/Nick/Dropbox/Work/Data Science/13 - Data for Good Datathon/scripts/LDA.analysis.R')
source("LDA.analysis.R")
source("topicmodels_json_ldavis.R")
source("LDA.analysis.R")
source("topicmodels_json_ldavis.R")
return_list =LDA.analysis(text_vector = dfnew,num_topics = 15)
return_list$topicLabel
return_list$topicTerms
source('C:/Users/Nick/Dropbox/Work/Data Science/13 - Data for Good Datathon/scripts/LDA.analysis.R')
lda.mod =LDA.analysis(text_vector = dfnew,num_topics = 15)
return_list$lda_model
topicmodels::terms(llis.model,1)
topicmodels::terms(return_list$lda_model,1)
topicmodels::terms(return_list$lda_model,2)
topicmodels::terms(return_list$lda_model)
posterior(return_list$lda_model)$terms
topicmodels::posterior(return_list$lda_model)$terms
n=topicmodels::posterior(return_list$lda_model)$terms
n[1]
dim(n)
?sort
sort(n)
dim(sort(n))
data.table(n)
require(data.table)
?posterior
p=data.table(n)
#Get term weights
require(data.table)
term_weights=topicmodels::posterior(return_list$lda_model)$terms
term_weight=data.table(term_weights)
head(term_weights)
term_weights[1,1]
term_weights[1,2]
term_weights[16,2]
term_weights[15,2]
data.table(x=rep(c("b","a","c"),each=3), y=c(1,3,6), v=1:9)
sort(data.table[1,])[1:10]
sort(term_weight[1,])[1:10]
sort(term_weight[,1])[1:10]
term_weights=topicmodels::posterior(return_list$lda_model)$terms
term_weight=data.table(term_weights)
dim(term_weight)
?data.table
sort(term_weight[1:2,1:2])[1:10]
sort(term_weight[1:2,1:2])
sort(term_weight[1:2,])
sort(term_weight[,1:2])
term_weight[1,1:5]
sort(term_weight[1,1:100])
sort(term_weight[1,1:100])[1]
sort(term_weight[1,1:100])[1,1]
size(n)
sim(n)
dim(n)
dim(n)[1]
for i=1:dim(n)[1] {
print(i)
}
for (i in 1:dim(n)[1]) {
print(i)
}
sort(term_weight[1,1:100])[1,1]
sort(term_weight[2,1:100])[1,1]
sort(term_weight[2,1:100],)[1,1]
?sort
sort(term_weight[2,1:100],decreasing=TRUE)[1,1]
sort(term_weight[2,1:100],decreasing=TRUE)[2,1]
sort(term_weight[2,1:100],decreasing=TRUE)[1,1]
sort(term_weight[2,1:100],decreasing=TRUE)[1,1:30]
sort(term_weight[2,],decreasing=TRUE)[1,1:30]
dfwc=data.frame()
return_list$topicTerms
return_list$topicTerms[,1]
source('C:/Users/Nick/Dropbox/Work/Data Science/13 - Data for Good Datathon/scripts/LDA.analysis.R')
lda_num_topics=15
lda.mod =LDA.analysis(text_vector = dfnew,num_topics = lda_num_topics)
#Get term weights
require(data.table)
term_weights=topicmodels::posterior(return_list$lda_model)$terms
term_weight=data.table(term_weights)
i=1
words=return_list$topicTerms[,i]
weights=sort(term_weight[2,],decreasing=TRUE)[1,1:30]
dfwc=data.frame(words,weights)
dfwc
weights
weights=sort(term_weight[i,],decreasing=TRUE)[1,1:30]
weights
words
dfwc=data.frame(words,weights)
dfwc
values(words)
words
weights
as.numerical(weights)
numerical(weights)
weights=sort(term_weight[i,],decreasing=TRUE)[1,1:30]
weights
weights=sort(term_weight[i,],decreasing=TRUE)[[1,1:30]]
weights
weights=sort(term_weight[i,],decreasing=TRUE)[[1:30]]
weights=sort(term_weight[i,],decreasing=TRUE)[1,1:30]
weights
weights[[]]
weights[[,]]
weights[[1:30]]
type(weights)
coefficients(weights)
coefficients(weights)[1]
coefficients(weights)[1,1]
unname(weights)[1,1]
unname(weights)
weights=unname(sort(term_weight[i,],decreasing=TRUE)[1,1:30])
dfwc=data.frame(words,weights)
weights
dim(weights)
dfwc=data.frame(words,t(weights)
sort(term_weight[2,],decreasing=TRUE)[1,1:30]
}
# docs <- Corpus(VectorSource(dfnew))
# docs <- tm_map(docs, removePunctuation)
# docs <- tm_map(docs, removeNumbers)
# docs <- tm_map(docs, tolower)
# docs <- tm_map(docs, removeWords, stopwords("english"))
# docs <- tm_map(docs, removeWords, stopwords("french"))
# docs <- tm_map(docs, stemDocument) #remove endings
# #docs <- tm_map(docs, removeWords, c("program","programs","services","service","provide", "community","support","calgary","alberta","canadian","canada","edmonton"))
# dtm <- DocumentTermMatrix(docs)
#
# #remove duplicate documents:
# ui = unique(dtm$i)
# dtm.new = dtm[ui,]
#
#
#
# # #Caculate optimal number of topics: http://davidmeza1.github.io/2015/07/20/topic-modeling-in-R.html
# library(Rmpfr)
# term_tfidf <- tapply(dtm$v/slam::row_sums(dtm)[dtm$i], dtm$j, mean) *log2(tm::nDocs(dtm)/slam::col_sums(dtm > 0))
# summary(term_tfidf)
# #
# #remove words
# llisreduced.dtm <- dtm[,term_tfidf >= 0.005]
# summary(slam::col_sums(llisreduced.dtm))
# #
# # harmonicMean <- function(logLikelihoods, precision = 2000L) {
# #   llMed <- median(logLikelihoods)
# #   as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
# #                                        prec = precision) + llMed))))
# # }
# # seqk <- seq(2, 100, 1)
# # burnin <- 1000
# # iter <- 1000
# # keep <- 50
# #
# # system.time(fitted_many <- lapply(seqk, function(k) topicmodels::LDA(llisreduced.dtm, k = k,
# #                                                                      method = "Gibbs",control = list(burnin = burnin,
# #                                                                                                      iter = iter, keep = keep) )))
# # logLiks_many <- lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
# # hm_many <- sapply(logLiks_many, function(h) harmonicMean(h))
# # optimal_topic_number <-  seqk[which.max(hm_many)]  #- optimal topics number
# optimal_topic_number <-15
# system.time(llis.model <- topicmodels::LDA(llisreduced.dtm, optimal_topic_number, method = "Gibbs", control = list(iter=2000, seed = 0622)))
#
#
# #Create labels for topics  - join 3 most common words
# library(tidyr)
# llis.topics <- topicmodels::topics(llis.model, 1)
# llis.terms <- as.data.frame(topicmodels::terms(llis.model, 30), stringsAsFactors = FALSE)
#
# topicTerms <- tidyr::gather(llis.terms, Topic)
# topicTerms <- cbind(topicTerms, Rank = rep(1:30))
# topTerms <- dplyr::filter(topicTerms, Rank < 5)
# topTerms <- dplyr::mutate(topTerms, Topic = stringr::word(Topic, 2))
# topTerms$Topic <- as.numeric(topTerms$Topic)
# topicLabel <- data.frame()
# for (i in 1:optimal_topic_number){
#   z <- dplyr::filter(topTerms, Topic == i)
#   l <- as.data.frame(paste(z[1,2], z[2,2], z[3,2],z[4,2], sep = " " ), stringsAsFactors = FALSE)
#   topicLabel <- rbind(topicLabel, l)
# }
#
# colnames(topicLabel) <- c("Label")
# topicLabel
#Match topics back to organization
theta <- as.data.frame(topicmodels::posterior(llis.model)$topics)
head(theta)
#Map Charity to each Topic
colnames(theta)<-t(topicLabel)
classification=colnames(theta)[max.col(theta,ties.method="first")]
dfn$CharityType=factor(classification, level=topicLabel$Label)
#Make Key and join dataframes
x <- as.data.frame(row.names(theta), stringsAsFactors = FALSE)
colnames(x) <- c("Key")
x$Key <- as.numeric(x$Key)
theta2 <- cbind(x, theta)
dfn$Key <- row.names(dfn)
dfn$Key<- as.numeric(dfn$Key)
dffull <- dplyr::left_join(dfn,theta2, by = "Key")
write.table(x=dffull,
file="ckc_calgary_mega_export.csv",
quote=TRUE,
sep=",")
dfwc=data.frame(words,t(weights))
dfwc
0.026138372/0.003769807
library(wordcloud)
install.packages('wordcloud')
dfwc
wordcloud(dfwc$words, dfwc$t.weights.)
library(wordcloud)
wordcloud(dfwc$words, dfwc$t.weights.)
library(ggrepel)
ggplot +
aes(x = 1, y = 1, size = t.weights., label = word) +
geom_text_repel(segment.size = 0, force = 100) +
scale_size(range = c(2, 15), guide = FALSE) +
scale_y_continuous(breaks = NULL) +
scale_x_continuous(breaks = NULL) +
labs(x = '', y = '') +
theme_classic()
dfwc %>%
slice(1:30) %>%
ggplot +
aes(x = 1, y = 1, size = t.weights., label = word) +
geom_text_repel(segment.size = 0, force = 100) +
scale_size(range = c(2, 15), guide = FALSE) +
scale_y_continuous(breaks = NULL) +
scale_x_continuous(breaks = NULL) +
labs(x = '', y = '') +
theme_classic()
#library(wordcloud)
library(ggplot2)
library(ggrepel)
#wordcloud(dfwc$words, dfwc$t.weights.)
dfwc %>%
slice(1:30) %>%
ggplot +
aes(x = 1, y = 1, size = t.weights., label = word) +
geom_text_repel(segment.size = 0, force = 100) +
scale_size(range = c(2, 15), guide = FALSE) +
scale_y_continuous(breaks = NULL) +
scale_x_continuous(breaks = NULL) +
labs(x = '', y = '') +
theme_classic()
#wordcloud(dfwc$words, dfwc$t.weights.)
dfwc %>%
ggplot +
aes(x = 1, y = 1, size = t.weights., label = word) +
geom_text_repel(segment.size = 0, force = 100) +
scale_size(range = c(2, 15), guide = FALSE) +
scale_y_continuous(breaks = NULL) +
scale_x_continuous(breaks = NULL) +
labs(x = '', y = '') +
theme_classic()
dfwc
#wordcloud(dfwc$words, dfwc$t.weights.)
dfwc %>%
ggplot +
aes(x = 1, y = 1, size = t.weights., label = words) +
geom_text_repel(segment.size = 0, force = 100) +
scale_size(range = c(2, 15), guide = FALSE) +
scale_y_continuous(breaks = NULL) +
scale_x_continuous(breaks = NULL) +
labs(x = '', y = '') +
theme_classic()
wordcloud(dfwc$words, dfwc$t.weights.,colors=brewer.pal(8, "Dark2"))
wordcloud(dfwc$words,
dfwc$t.weights.,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
brewer.pal()
paste(i, ".jpg")
paste(i, ".jpg", sep="")
for (i in 1:dim(n)[1]) {
words=return_list$topicTerms[,i]
weights=unname(sort(term_weight[i,],decreasing=TRUE)[1,1:30])
dfwc=data.frame(words,t(weights))
png(filename = paste(i, ".jpg", sep=""))
wordcloud(dfwc$words,
dfwc$t.weights.,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
dev.off()
}
setwd('C:\\Users\\Nick\\Dropbox\\Work\\Data Science\\13 - Data for Good Datathon\\scripts\\datathon2017')
setwd('C:\\Users\\Nick\\Dropbox\\Work\\Data Science\\13 - Data for Good Datathon\\scripts\\datathon2017')
for (i in 1:dim(n)[1]) {
words=return_list$topicTerms[,i]
weights=unname(sort(term_weight[i,],decreasing=TRUE)[1,1:30])
dfwc=data.frame(words,t(weights))
png(filename = paste(i, ".png", sep=""))
wordcloud(dfwc$words,
dfwc$t.weights.,
random.order=FALSE,
colors=brewer.pal(8, "Dark2"))
dev.off()
}
i=4
words=return_list$topicTerms[,i]
words
weights=unname(sort(term_weight[i,],decreasing=TRUE)[1,1:30])
weights
0.0375140/0.003396395
i=1
words=return_list$topicTerms[,i]
weights=unname(sort(term_weight[i,],decreasing=TRUE)[1,1:30])
weights
